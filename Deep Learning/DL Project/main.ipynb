{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Colorization using Autoencoder\n",
    "Deep Learning model written in Pytorch to convert gray scaled images into RGB/LAB images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import Libraries\n",
    "Convert images to grayscale and sort them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "\n",
    "import wandb\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input and Output folder for color transformation\n",
    "input_folder = './color_train/'\n",
    "input_color = './data/color_train/'\n",
    "output_folder = './data/gray_train/'\n",
    "original_bw_folder = './data/original_bw_images'\n",
    "num_img_to_process = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_image(image_path, color_mode='rgb', target_size=(160, 160)):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert color mode if necessary\n",
    "    if color_mode == 'rgb':\n",
    "        image = image.convert('RGB')\n",
    "    elif color_mode == 'grayscale':\n",
    "        image = image.convert('L')\n",
    "    \n",
    "    # Resize the image\n",
    "    resize_transform = transforms.Resize(target_size)\n",
    "    image = resize_transform(image)\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    image = transforms.ToTensor()(image)  # Converts to [0, 1] range\n",
    "    if color_mode == 'rgb':\n",
    "        image = transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])(image)  # Normalize\n",
    "    \n",
    "    # Ensure grayscale images are converted to RGB format\n",
    "    if color_mode == 'grayscale':\n",
    "        image = image.expand(3, -1, -1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Classes for sorting the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining class of Image DataSet:\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, color_mode='rgb', target_size=(160, 160)):\n",
    "        self.image_paths = image_paths\n",
    "        self.color_mode = color_mode\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "\n",
    "        # Open image to check if it matches the expected color mode\n",
    "        with Image.open(image_path) as img:\n",
    "            img_mode = img.mode\n",
    "            \n",
    "            if self.color_mode == 'grayscale' and img_mode != 'L':\n",
    "                print(f\"Warning: {image_path} is expected to be grayscale (mode 'L'), but it is {img_mode}.\")\n",
    "                warning_flag = True\n",
    "\n",
    "            elif self.color_mode == 'rgb' and img_mode != 'RGB':\n",
    "                print(f\"Warning: {image_path} is expected to be RGB (mode 'RGB'), but it is {img_mode}.\")\n",
    "                warning_flag = True\n",
    "            else:\n",
    "                warning_flag = False\n",
    "  \n",
    "        # Load the image using the desired color mode\n",
    "        image = load_image(image_path, color_mode=self.color_mode, target_size=self.target_size)\n",
    "\n",
    "        if warning_flag:\n",
    "            print(f\"Warning - Images located incorrectly: {image_path}\")\n",
    "\n",
    "        return image\n",
    "\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, grayscale_dataset, color_dataset):\n",
    "        assert len(grayscale_dataset) == len(color_dataset), \"Datasets must have the same length.\"\n",
    "        self.grayscale_dataset = grayscale_dataset\n",
    "        self.color_dataset = color_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grayscale_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        grayscale_image = self.grayscale_dataset[idx]\n",
    "        color_image = self.color_dataset[idx]\n",
    "        return grayscale_image, color_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Loading images\n",
    "Converting RGB images from input folder to grayscale images and saving them in output folder. \n",
    "Additionally, it filters out original grayscale images, saving it into \"original_bw_images\" folder to evaluate later on. (Ground truth unknown here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 images in folder ./color_train/.\n",
      "Images converted into grayscale and saved in ./data/gray_train/.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Output Folder if it does not exist yet\n",
    "os.makedirs(input_color, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(original_bw_folder, exist_ok=True)\n",
    "\n",
    "# Take num_img_to_process images to convert to grayscale\n",
    "\n",
    "input_all = os.listdir(input_folder)\n",
    "num_images = min(num_img_to_process, len(input_all))\n",
    "image_files = [f for f in input_all[:num_images] if f.endswith(('jpg', 'jpeg', 'png'))]\n",
    "\n",
    "print(f\"Collected {len(image_files)} images in folder {input_folder}.\")\n",
    "\n",
    "# Itera su ogni immagine, convertila in grayscale e salvala\n",
    "for image_file in image_files:\n",
    "    input_path = os.path.join(input_folder, image_file)\n",
    "    output_path = os.path.join(output_folder, image_file)\n",
    "    originals_path = os.path.join(original_bw_folder, image_file)\n",
    "    # Apri l'immagine e converti in grayscale\n",
    "    with Image.open(input_path) as img:\n",
    "        # Original Grayscale images in Coco Dataset:\n",
    "        if img.mode == 'L':\n",
    "            img.save(originals_path)\n",
    "            os.remove(input_path)\n",
    "            print(input_path)\n",
    "        else:\n",
    "            grayscale_img = img.convert(\"L\")  # Converti in grayscale\n",
    "            grayscale_img.save(output_path)\n",
    "\n",
    "print(f\"Images converted into grayscale and saved in {output_folder}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and pairing dataset\n",
    "\n",
    "Sorting dataset to train and test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs created: 100\n",
      "train len: 85, test len:15\n"
     ]
    }
   ],
   "source": [
    "# Define paths to rgb and gray scale images\n",
    "color_images_path = input_folder\n",
    "grayscale_images_path = output_folder\n",
    "\n",
    "# Get sorted lists of image file paths\n",
    "color_image_files = sorted([os.path.join(color_images_path, f) for f in os.listdir(color_images_path) if f.endswith(('jpg', 'png', 'jpeg'))])\n",
    "grayscale_image_files = sorted([os.path.join(grayscale_images_path, f) for f in os.listdir(grayscale_images_path) if f.endswith(('jpg', 'png', 'jpeg'))])\n",
    "\n",
    "# Ensure the filenames without paths are used for matching\n",
    "color_filenames = {os.path.basename(f): f for f in color_image_files}\n",
    "grayscale_filenames = {os.path.basename(f): f for f in grayscale_image_files}\n",
    "\n",
    "# Match pairs based on filenames\n",
    "paired_filenames = [(grayscale_filenames[f], color_filenames[f]) for f in grayscale_filenames if f in color_filenames]\n",
    "\n",
    "# Check if pairs were created correctly\n",
    "print(f\"Number of pairs created: {len(paired_filenames)}\")\n",
    "\n",
    "# Create a dataset from the paired filenames\n",
    "grayscale_paths, color_paths = zip(*paired_filenames)\n",
    "\n",
    "grayscale_paths = list(grayscale_paths)\n",
    "color_paths = list(color_paths)\n",
    "\n",
    "# Split the dataset into train and test sets (train = 85%, test = 15%)\n",
    "train_size  = int(len(grayscale_paths)*0.85)\n",
    "test_size = int(len(grayscale_paths)*0.15)\n",
    "print(f\"train len: {train_size}, test len:{test_size}\")\n",
    "\n",
    "train_grayscale_paths = grayscale_paths[:train_size]\n",
    "test_grayscale_paths = grayscale_paths[test_size:]\n",
    "\n",
    "train_color_paths = color_paths[:train_size]\n",
    "test_color_paths = color_paths[test_size:]\n",
    "\n",
    "# Creating datasets\n",
    "## train dataset\n",
    "train_grayscale_ds = ImageDataset(train_grayscale_paths, color_mode='grayscale')\n",
    "train_color_ds = ImageDataset(train_color_paths, color_mode='rgb')\n",
    "\n",
    "## test dataset\n",
    "test_grayscale_ds = ImageDataset(test_grayscale_paths, color_mode='grayscale')\n",
    "test_color_ds = ImageDataset(test_color_paths, color_mode='rgb')\n",
    "\n",
    "# Ensure there are no mix-ups in dataset - rgb/grayscale separation\n",
    "for idx in range(len(train_grayscale_ds)):\n",
    "    image = train_grayscale_ds[idx]\n",
    "\n",
    "for idx in range(len(train_color_ds)):\n",
    "    image = train_color_ds[idx] \n",
    "\n",
    "for idx in range(len(test_grayscale_ds)):\n",
    "    image = test_grayscale_ds[idx] \n",
    "\n",
    "for idx in range(len(test_color_ds)):\n",
    "    image = test_color_ds[idx]\n",
    "\n",
    "# Creating data loaders\n",
    "train_grayscale_loader = DataLoader(train_grayscale_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "train_color_loader = DataLoader(train_color_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "test_grayscale_loader = DataLoader(test_grayscale_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_color_loader = DataLoader(test_color_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Combine grayscale and color datasets into paired datasets\n",
    "train_dataset = PairedDataset(train_grayscale_ds, train_color_ds)\n",
    "test_dataset = PairedDataset(test_grayscale_ds, test_color_ds)\n",
    "\n",
    "# DataLoader configurations\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# # Train the model using the train dataset\n",
    "# model.fit(train_dataset, epochs=50, validation_data=test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "## Define model\n",
    "Here we define the encoder/decoder (down/up) building blocks of the Autoencoder.\n",
    "* down function reduces the spatial dimension of the input\n",
    "* up function increases the spatial dimension, reconstructing the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down function\n",
    "def down(filters, kernel_size, apply_batch_normalization=True):\n",
    "    layers = []\n",
    "    \n",
    "    # Conv2d layer\n",
    "    layers.append(nn.Conv2d(in_channels=filters, out_channels=filters, kernel_size=kernel_size, stride=2, padding=1))\n",
    "    \n",
    "    # Batch Normalization\n",
    "    if apply_batch_normalization:\n",
    "        layers.append(nn.BatchNorm2d(filters))\n",
    "    \n",
    "    # LeakyReLU activation\n",
    "    layers.append(nn.LeakyReLU(negative_slope=0.2))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# Up function\n",
    "def up(filters, kernel_size, dropout=False):\n",
    "    layers = []\n",
    "    \n",
    "    # ConvTranspose2d layer\n",
    "    layers.append(nn.ConvTranspose2d(in_channels=filters, out_channels=filters, kernel_size=kernel_size, stride=2, padding=1))\n",
    "    \n",
    "    # Dropout\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout(0.2))\n",
    "    \n",
    "    # LeakyReLU activation\n",
    "    layers.append(nn.LeakyReLU(negative_slope=0.2))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (d1): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (d2): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (d3): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (d4): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (d5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (u1): Sequential(\n",
      "    (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (u2): Sequential(\n",
      "    (0): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (u3): Sequential(\n",
      "    (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (u4): Sequential(\n",
      "    (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (u5): Sequential(\n",
      "    (0): ConvTranspose2d(3, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (final_conv): Conv2d(6, 3, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Downsampling layers\n",
    "        self.d1 = down(128, (3, 3), False)\n",
    "        self.d2 = down(128, (3, 3), False)\n",
    "        self.d3 = down(256, (3, 3), True)\n",
    "        self.d4 = down(512, (3, 3), True)\n",
    "        self.d5 = down(512, (3, 3), True)\n",
    "\n",
    "        # Upsampling layers\n",
    "        self.u1 = up(512, (3, 3), False)\n",
    "        self.u2 = up(256, (3, 3), False)\n",
    "        self.u3 = up(128, (3, 3), False)\n",
    "        self.u4 = up(128, (3, 3), False)\n",
    "        self.u5 = up(3, (3, 3), False)\n",
    "\n",
    "        # Final convolution layer\n",
    "        self.final_conv = nn.Conv2d(6, 3, kernel_size=2, stride=1, padding=1)  # 6 channels from concatenation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        d1 = self.d1(x)\n",
    "        d2 = self.d2(d1)\n",
    "        d3 = self.d3(d2)\n",
    "        d4 = self.d4(d3)\n",
    "        d5 = self.d5(d4)\n",
    "\n",
    "        # Upsampling path\n",
    "        u1 = self.u1(d5)\n",
    "        u1 = torch.cat([u1, d4], dim=1)  # Concatenate along channels\n",
    "        u2 = self.u2(u1)\n",
    "        u2 = torch.cat([u2, d3], dim=1)\n",
    "        u3 = self.u3(u2)\n",
    "        u3 = torch.cat([u3, d2], dim=1)\n",
    "        u4 = self.u4(u3)\n",
    "        u4 = torch.cat([u4, d1], dim=1)\n",
    "        u5 = self.u5(u4)\n",
    "        u5 = torch.cat([u5, x], dim=1)  # Concatenate the input image\n",
    "\n",
    "        # Final output\n",
    "        output = self.final_conv(u5)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "model = UNet()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "color-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
